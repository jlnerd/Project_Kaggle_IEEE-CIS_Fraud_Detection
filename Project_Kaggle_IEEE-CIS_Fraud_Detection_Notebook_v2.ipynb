{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, we benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. We also have the opportunity to create new features to improve your results.\n",
    "\n",
    "See the [README](README.md) for further details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we leverage a custom API, [JLpyUtils](https://pypi.org/project/JLpyUtils/), the author ([John Leonard](https://www.linkedin.com/in/johntleonard/)) has developed to streamline exploritory data analysis, feature engineering, and model selection tasks. Furthermore, we heavily utilize the [dask](https://dask.org), as it is much more efficient at managing large datasets such as those used in this analysis. To further improve our memory resource management, we often use python manual garbage collection function ```gc.collect()``` to clear out deleted objects from memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install JLpyUtils==0.2.9\n",
    "!pip install tensorflow==1.14.0\n",
    "!pip install tensorflow-gpu==1.14.0\n",
    "!pip install dask_ml\n",
    "\n",
    "import IPython.display\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, importlib, gc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import dask_xgboost\n",
    "import dask_ml, dask_ml.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000\n",
    "mpl.rcParams['font.size']=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_desktop = '/mr3_boltprod_john_t_leonard/Data_Science_Projects.'\n",
    "if dev:\n",
    "    print('Running in dev mode. Using local copy of JLpyUtils')\n",
    "    path_dev_repo = os.path.join(path_desktop,'JLpyUtils')\n",
    "    sys.path.insert(0, path_dev_repo)\n",
    "\n",
    "import JLpyUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JLpyUtils.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competion uses train test sets that are >100 mb, which is githubs standard limit, thus we don't store the dataset directly in the repo.. Below, we download the data from kaggle URL specified at the competition home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_data_dir = 'ieee-fraud-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "\n",
    "# files = api.competition_download_files(\"ieee-fraud-detection\", path = path_raw_data_dir)\n",
    "\n",
    "# import zipfile\n",
    "# for file in os.listdir(path_raw_data_dir):\n",
    "#     if 'zip' in file:\n",
    "#         with zipfile.ZipFile(os.path.join(path_raw_data_dir, file) , 'r') as zip_ref:\n",
    "#             zip_ref.extractall(path_raw_data_dir)\n",
    "#         os.remove(os.path.join(path_raw_data_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path_raw_data_dir,\n",
    "            train_test_id = 'train'):\n",
    "    \"\"\"\n",
    "    Load the train or test df by left joining the transaction data with the identity data on the 'TransactionID' header\n",
    "    \"\"\"\n",
    "    import gc \n",
    "    \n",
    "    import dask.dataframe as dd\n",
    "    \n",
    "    df_transaction = dd.read_csv(os.path.join( path_raw_data_dir, train_test_id+'_transaction.csv'))\n",
    "    df_identity = dd.read_csv(os.path.join( path_raw_data_dir, train_test_id+'_identity.csv'))\n",
    "    \n",
    "    df = dd.merge(df_transaction, df_identity, how='left', on = 'TransactionID')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    del df_transaction, df_identity\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "df = load_df(path_raw_data_dir, train_test_id = 'train')\n",
    "display(df.info())\n",
    "display(df.head(), df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Feature and Label headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_headers_dict():\n",
    "    headers_dict = {'labels':['isFraud'],\n",
    "                    'UID':'TransactionID'}\n",
    "    headers_dict['features'] = list(df.drop(columns=headers_dict['labels']+ [headers_dict['UID']]).columns)\n",
    "    headers_dict['categorical features'] = ['ProductCD']+[header for header in headers_dict['features'] if 'card' in header or 'addr' in header or 'emaildomain' in header or 'M' in header or 'Device' in header and header != headers_dict['UID']] + ['id_'+str(int_) for int_ in range(12,39)]\n",
    "    headers_dict['continuous features'] = [feature for feature in headers_dict['features'] if feature not in headers_dict['categorical features']]\n",
    "    return headers_dict\n",
    "\n",
    "headers_dict = fetch_headers_dict()\n",
    "for key in headers_dict.keys():\n",
    "    print('\\n',key,':', headers_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[headers_dict['features']]\n",
    "y = df[headers_dict['labels']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feature Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_feat_cleaner(X):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    X['M1'] = X['M1'].fillna('F')\n",
    "    \n",
    "    warnings.filterwarnings('default')\n",
    "    return X\n",
    "\n",
    "X = basic_feat_cleaner(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Subset of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.head(1000)\n",
    "y = y.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JLpyUtils.plot.hist_or_bar(X, categorical_headers= headers_dict['categorical features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JLpyUtils.plot.hist_or_bar(y, categorical_headers= ['isFraud'], n_plot_columns=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(JLpyUtils)\n",
    "importlib.reload(JLpyUtils.file_utils)\n",
    "importlib.reload(JLpyUtils.ML)\n",
    "importlib.reload(JLpyUtils.ML.preprocessing)\n",
    "importlib.reload(JLpyUtils.ML.preprocessing.LabelEncode)\n",
    "importlib.reload(JLpyUtils.ML.preprocessing.Scale)\n",
    "importlib.reload(JLpyUtils.ML.preprocessing.Impute)\n",
    "importlib.reload(JLpyUtils.ML.preprocessing.OneHotEncode)\n",
    "\n",
    "df = load_df(path_raw_data_dir, train_test_id = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice out subset for code dev\n",
    "df = df.partitions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[headers_dict['features']]\n",
    "y = df[headers_dict['labels']]\n",
    "\n",
    "X = basic_feat_cleaner(X)\n",
    "\n",
    "display(X.head())\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate The Feature Engineering Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```JLpyUtils.ML.preprocessing.feat_eng_pipe``` class is designed to streamline & automate running various feature engineering operations. The feature engineering sequence is:\n",
    "1. LabelEncode.categorical_features\n",
    "2. Scale.continuous_features\n",
    "    * for Scaler_ID in Scalers_dict.keys()\n",
    "3. Impute.categorical_features\n",
    "    * for Imputer_cat_ID in Imputer_categorical_dict[Imputer_cat_ID].keys():<br>\n",
    "        *for Imputer_iter_class_ID in Imputer_categorical_dict[Imputer_cat_ID].keys():\n",
    "4. Imputer.continuous_features\n",
    "    * for Imputer_cont_ID in Imputer_continuous_dict.keys():\n",
    "        * for Imputer_iter_reg_ID in Imputer_continuous_dict[Imputer_cont_ID].keys():\n",
    "5. OneHotEncode\n",
    "6. CorrCoeffThreshold\n",
    "\n",
    "For many of these operations, there are various hyperparameters that could be varied to perform similar but different types of feature engineering. The default settings in the class are setup to allow one to ultimately perform model training on data sets that have different types of scaling, or imputation, etc.. applied. However, for simplicy, and because this dataset is quite large, we will just focus on one feature engineering pipe scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_eng_pipe = JLpyUtils.ML.preprocessing.feat_eng_pipe(path_report_dir = path_desktop, \n",
    "                                                              verbose=1, \n",
    "                                                              overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Single Feature engineering case to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could iterate through all possible feature engineering scenarios, but this dataset is quite large and that would take quite a bit of time, so we will just evaluate one promising scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "\n",
    "feat_eng_pipe.Scalers_dict = {'MinMaxScaler': sklearn.preprocessing.MinMaxScaler()}\n",
    "feat_eng_pipe.Imputer_categorical_dict = {'most_frequent': {None: None}}\n",
    "feat_eng_pipe.Imputer_continuous_dict = {'median':{None:None}}#{ 'iterative': {'BayesianRidge': sklearn.linear_model.BayesianRidge()}}\n",
    "feat_eng_pipe.OneHot_cases = [True]\n",
    "feat_eng_pipe.AbsCorrCoeff_thresholds = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Feat Eng Pipe on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_eng_pipe.fit(X, headers_dict=headers_dict, format_='csv')\n",
    "\n",
    "del X\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform X_field using Feat Eng Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call Kaggle's \"test\" data \"field\" data, since usually you actually have labels in the \"test\" data for machine learning problems, and since we don't actually have access to the labels, it's kinda more like testing our model on field data and getting feedback later on have good or bad it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field = load_df(path_raw_data_dir, train_test_id = 'test')\n",
    "X_field = df_field[headers_dict['features']]\n",
    "\n",
    "X_field = basic_feat_cleaner(X_field)\n",
    "\n",
    "del df_field\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dev: #slice out 1 partition of the data for development\n",
    "    X_field = X_field.partitions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_eng_pipe.transform(X_field)\n",
    "\n",
    "del X_field\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Necessary Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feat_eng_case_dir in feat_eng_pipe.path_feat_eng_dirs:\n",
    "feat_eng_case_dir = os.path.join(feat_eng_pipe.path_feat_eng_root_dir, \n",
    "                                 'LabelEncode/Scaler_ID[MinMaxScaler]',\n",
    "                                 'Imputer_categorical_ID[most_frequent]',\n",
    "                                 'Imputer_iterator_classifier_ID[None]',\n",
    "                                 'Imputer_continuous_ID[median]',\n",
    "                                 'Imputer_iterator_regressor_ID[None]',\n",
    "                                 'OneHot_case[True]',\n",
    "                                 'CorrCoeffThreshold[1]')\n",
    "feat_eng_case_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_dict = JLpyUtils.file_utils.load('headers_dict','json',feat_eng_case_dir)\n",
    "headers_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = JLpyUtils.file_utils.load('X','csv', feat_eng_case_dir, headers=headers_dict['headers after CorrCoeffThreshold'])\n",
    "\n",
    "#ensure column headers of formatting appropriate for xgboost\n",
    "columns_reformatted = [col.replace('[','(').replace(']',')').replace('<','less') for col in X.columns]\n",
    "X.columns = columns_reformatted\n",
    "\n",
    "display(X.head())\n",
    "\n",
    "# ensure X & y have consistant partitions\n",
    "y = y.repartition(npartitions=X.npartitions)\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.merge(y, left_index = True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train, Xy_test = Xy.random_split([0.7, 0.3],\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Xy_train[X.columns]\n",
    "X_test = Xy_test[X.columns]\n",
    "\n",
    "y_train = Xy_train[y.columns]\n",
    "y_test = Xy_test[y.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be added at later time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#try shutting down client to ensure you don't start 2 clients\n",
    "try:\n",
    "    client.close()\n",
    "except:\n",
    "    None\n",
    "\n",
    "#start client server\n",
    "client = dask.distributed.Client()\n",
    "display(client)b\n",
    "\n",
    "params = {'objective': 'binary:logistic',\n",
    "          'max_depth': 4, 'eta': 0.01, 'subsample': 0.5,\n",
    "          'min_child_weight': 0.5}\n",
    "\n",
    "#run training & close client if something goes wrong.\n",
    "try:\n",
    "     model = dask_xgboost.train(client, params, X_train, y_train, num_boost_round=1)\n",
    "    client.close()\n",
    "except Exception as e:\n",
    "    client.close()\n",
    "    raise e\n",
    "    \n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgboost.plot_importance(model)\n",
    "ax.grid(which='both', visible=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
